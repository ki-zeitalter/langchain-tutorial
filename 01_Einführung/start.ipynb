{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einführung\n",
    "\n",
    "Im diesem Teil des Tutorials zeige ich dir die ersten Schritte mit LangChain. Dabei installieren wir die notwendigen Bibliotheken und schreiben den allerersten Basiscode um mit einem Sprachmodell wie ChatGPT (OpenAI) zu kommunizieren.\n",
    "\n",
    "\n",
    "# Warum LangChain?\n",
    "\n",
    "Die Anbieter von KIs (Sprachmodellen/LLMs) bieten häufig eigene APIs an, mit denen man mit die KI in die eigene Anwendung integrieren kann. So kann man z.B. mit der API von OpenAI bereits sehr einfach die ersten Aufrufe machen.\n",
    "Damit wir uns nicht an die API der jeweiligen Anbieter binden müssen, bietet uns LangChain uns eine einheitliche API um mit allen möglichen LLMs zu kommunizieren. Je nach Anwendungsfall können wir sogar die verschiedenen LLMs in einer Anwendung verwenden. So können wir mit einem Open-Source-Modell sog. Embeddings erstellen (mehr dazu in einem anderen Kapitel) und die Chat-Funktion von ChatGPT (OpenAI) nutzen. Wir können auch jederzeit das verwendete LLM austauschen, ohne den Code neu schreiben zu müssen.\n",
    "Zusätzlich dazu bietet LangChain vielfältige weitere Möglichkeiten. Mit den PromptTemplates können wir die Erstellung und Verwaltung von Prompts (Eingaben für die LLMs) bewerkstelligen, ohne irgendwelche Strings zusammen zu basteln. Wir können mit den Output Parsern die Ausgaben der LLMs maschinell auswerten und z.B. uns das Ergebnis als JSON zurück geben zu lassen, mit dem wir in unserem Programm arbeiten können. Spannend wird es spätestens dann, wenn wir die Agenten \"erwecken\". D.h. wir geben der KI Werkzeuge, mit denen die KI mehr als nur Texte erzeugen kann. Z.B. Informationen beschaffen, Berechnungen durchführen uvm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation notwendiger Bibliotheken\n",
    "\n",
    "Für die meisten Beispiele dieses Tutorial brauchen wir einige Bibliotheken. Wir brauchen natürlich die LangChain-Bibliothek. Daneben brauchen wir die OpenAI Bibliothek um mit ChatGPT zu kommunizieren (direkt aber auch unter Verwendung von LangChain). Dann benötigen wir außerdem Python Dotenv damit wir den notwendigen API-Key aus einer .env Datei laden können. Das hat den Vorteil, dass wir den geheimen API-Key nicht direkt im Code hinterlegen müssen und statt dessen in der .env Datei speichern. Diese Datei wird nicht im Git-Repository eingecheckt (in .gitignore ist diese Datei hinterlegt) und wir somit nicht aus versehen unseren API-Key veröffentlichen.\n",
    "\n",
    "Führe zunächst folgenden Befehl aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /usr/local/python/3.10.8/lib/python3.10/site-packages (0.0.310)\n",
      "Requirement already satisfied: openai in /usr/local/python/3.10.8/lib/python3.10/site-packages (0.28.1)\n",
      "Requirement already satisfied: python-dotenv in /usr/local/python/3.10.8/lib/python3.10/site-packages (1.0.0)\n",
      "Requirement already satisfied: tiktoken in /usr/local/python/3.10.8/lib/python3.10/site-packages (0.5.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/codespace/.local/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from langchain) (2.0.21)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from langchain) (3.8.5)\n",
      "Requirement already satisfied: anyio<4.0 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from langchain) (3.7.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from langchain) (0.6.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.40 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from langchain) (0.0.43)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/codespace/.local/lib/python3.10/site-packages (from langchain) (1.26.0)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from langchain) (2.4.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/codespace/.local/lib/python3.10/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.10.8/lib/python3.10/site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/codespace/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /home/codespace/.local/lib/python3.10/site-packages (from anyio<4.0->langchain) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/codespace/.local/lib/python3.10/site-packages (from anyio<4.0->langchain) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /home/codespace/.local/lib/python3.10/site-packages (from anyio<4.0->langchain) (1.1.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/codespace/.local/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/codespace/.local/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (4.8.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/codespace/.local/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain openai python-dotenv tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beispiel: Direkte Verwendung der OpenAI-Bibliothek\n",
    "\n",
    "Bevor wir mit LangChain starten, erstmal ein Beispiel dafür, wie man die OpenAI (ChatGPT) API direkt nutzen. Das ermöglicht uns die Verwendung zu vergleichen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-89WS7MWNBxYJFi37A9OljmDl51yx8\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1697280391,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Um gute Eingaben f\\u00fcr ChatGPT zu schreiben, solltest du die folgenden Punkte beachten:\\n\\n1. Klarheit der Anfrage: Stelle sicher, dass deine Anfrage pr\\u00e4zise und verst\\u00e4ndlich formuliert ist. Vermeide Doppeldeutigkeiten oder mehrdeutige Formulierungen, um Missverst\\u00e4ndnisse zu vermeiden.\\n\\n2. Kontext bereitstellen: Gib gen\\u00fcgend Hintergrundinformationen, damit ChatGPT den Kontext deiner Anfrage besser verstehen kann. Nenne wichtige Details, die f\\u00fcr die Beantwortung deiner Frage relevant sind.\\n\\n3. Anweisungen geben: Falls erforderlich, gib klare Anweisungen, wie ChatGPT deine Anfrage behandeln soll. Beispielsweise kannst du um eine Erkl\\u00e4rung bitten, einen Vergleich anstellen oder eine Meinung geben lassen.\\n\\n4. Grenzen setzen: Bei Bedarf kannst du direkte Anweisungen geben, damit ChatGPT den gew\\u00fcnschten Inhalt besser filtern kann. Du kannst zum Beispiel um Fakten, aktuelle Informationen oder pers\\u00f6nliche Erfahrungen bitten.\\n\\n5. Probeeingaben verwenden: Experimentiere mit verschiedenen Eingaben, um das Verhalten von ChatGPT besser zu verstehen. Lass dich nicht entmutigen, wenn die erste Antwort nicht zufriedenstellend ist. Iteriere und verbessere deine Eingabe, um die gew\\u00fcnschten Ergebnisse zu erzielen.\\n\\n6. Feedback geben: Wenn du auf unerw\\u00fcnschte oder falsche Antworten st\\u00f6\\u00dft, kannst du Feedback \\u00fcber das Daumen-hoch- oder Daumen-runter-Symbol geben. Dies hilft dabei, das Modell zu verbessern und zuk\\u00fcnftige R\\u00fcckmeldungen zu ber\\u00fccksichtigen.\\n\\nEs ist wichtig zu beachten, dass ChatGPT nicht perfekt ist und manchmal ungenaue oder irref\\u00fchrende Informationen liefern kann. Daher ist es ratsam, die Antworten kritisch zu betrachten und bei Bedarf externe Quellen zu pr\\u00fcfen.\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 27,\n",
      "    \"completion_tokens\": 443,\n",
      "    \"total_tokens\": 470\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"Erkläre mir bitte, wie ich gute Eingaben für ChatGPT schreibe.\"}]\n",
    "\n",
    "response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=messages)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir sehen, dass wir mit wenigen Zeilen Code eine Anfrage an ChatGPT senden können. Dabei fallen ein paar Sachen auf. Das erste ist, dass wir eine Liste von Nachrichten (_messages_) an die API übergeben. Außerdem enthält die Antwort wesentlich mehr, als nur den reinen Text der Antwort.\n",
    "\n",
    "Der Grund für die Liste der Nachrichten ist die, dass wir die Chat-Funktionalität nutzen. Der Server behält sich nicht die vorher eingegebenen Fragen und Antworten. D.h. man muss stets den gesamten Chat-Verlauf inkl. Antworten zurück übermitteln. Dadurch weiß die KI über welche Themen wir zuvor \"gesprochen\" haben. \n",
    "D.h. wenn ich jetzt eine Frage stellen möchte, die sich auf den bisherigen Chatverlauf bezieht, muss ich die Nachrichten-Liste erweitern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-89WWvBPhczZHFFNgBnOhDSXqUFF5Y\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1697280689,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Nat\\u00fcrlich! Hier sind die wesentlichen Punkte in einer kurzen Liste zusammengefasst:\\n\\n1. Klare und verst\\u00e4ndliche Formulierung der Anfrage.\\n2. Bereitstellung des Kontexts f\\u00fcr eine bessere Verst\\u00e4ndnisgrundlage.\\n3. Geben von klaren Anweisungen, wenn notwendig.\\n4. Setzen von Grenzen, um den gew\\u00fcnschten Inhalt zu filtern.\\n5. Experimentieren mit verschiedenen Eingaben, um das Verhalten von ChatGPT zu verstehen.\\n6. Geben von Feedback \\u00fcber das Daumen-hoch- oder Daumen-runter-Symbol.\\n\\nDiese Schritte helfen dabei, gute Eingaben f\\u00fcr ChatGPT zu erstellen und das gew\\u00fcnschte Ergebnis zu erzielen.\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 503,\n",
      "    \"completion_tokens\": 163,\n",
      "    \"total_tokens\": 666\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Die Antwort der KI muss als Nachricht in die Liste hinten angehängt werden\n",
    "messages.append(response[\"choices\"][0][\"message\"]);\n",
    "\n",
    "# Die neue Frage kommt noch dazu\n",
    "messages.append({\"role\": \"user\", \"content\": \"Danke für die Erklärung. Kannst du bitte die wesentlichen Punkte als kurze Liste zusammen fassen\"})\n",
    "\n",
    "# Erneuter Aufruf\n",
    "response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=messages)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Natürlich! Hier sind die wesentlichen Punkte in einer kurzen Liste zusammengefasst:\n",
       "\n",
       "1. Klare und verständliche Formulierung der Anfrage.\n",
       "2. Bereitstellung des Kontexts für eine bessere Verständnisgrundlage.\n",
       "3. Geben von klaren Anweisungen, wenn notwendig.\n",
       "4. Setzen von Grenzen, um den gewünschten Inhalt zu filtern.\n",
       "5. Experimentieren mit verschiedenen Eingaben, um das Verhalten von ChatGPT zu verstehen.\n",
       "6. Geben von Feedback über das Daumen-hoch- oder Daumen-runter-Symbol.\n",
       "\n",
       "Diese Schritte helfen dabei, gute Eingaben für ChatGPT zu erstellen und das gewünschte Ergebnis zu erzielen."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Schönere Ausgabe \n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(response[\"choices\"][0][\"message\"]['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit könnten wir schon eine Komplette Chat-Anwendung programmieren. Einfach Eingabe vom Benutzer holen, der Liste der Nachrichten hinzufügen, Aufruf ChatGPT, Antwort hinzufügen und das Ganze nochmal wiederholen.\n",
    "\n",
    "Lasst uns jetzt erste Schritte mit LangChain machen. Wir werden später sehen, wo uns LangChain hilft, diese Schritte zu automatisieren und wesentlich bequemer und übersichtlicher zu gestalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ja, du sprichst mit ChatGPT. Wie kann ich Ihnen helfen?\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "response = llm.predict(\"Spreche ich mit ChatGPT?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vom Code her, sind die Unterschiede zunächst nicht groß. Die Funktionen heißen etwas anders aber ansonsten wirkt das Ganze recht ähnlich. Der Unterschied ist jedoch, dass wir jederzeit das LLM austauschen können und anstelle von _ChatOpenAI_ eine andere LLM-Implementierung verwenden.\n",
    "\n",
    "In den kommenden Kapiteln sehen wir dann, wie LangChain uns die Arbeit an vielen Stellen abnimmt und uns ermöglicht das Programm sauber und übersichtlich zu halten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
