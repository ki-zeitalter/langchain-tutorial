{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einführung\n",
    "\n",
    "Im diesem Teil des Tutorials zeige ich dir die ersten Schritte mit LangChain. Dabei installieren wir die notwendigen Bibliotheken und schreiben den allerersten Basiscode um mit einem Sprachmodell wie ChatGPT (OpenAI) zu kommunizieren.\n",
    "\n",
    "\n",
    "# Warum LangChain?\n",
    "\n",
    "Die Anbieter von KIs (Sprachmodellen/LLMs) bieten häufig eigene APIs an, mit denen man mit die KI in die eigene Anwendung integrieren kann. So kann man z.B. mit der API von OpenAI bereits sehr einfach die ersten Aufrufe machen.\n",
    "Damit wir uns nicht an die API der jeweiligen Anbieter binden müssen, bietet uns LangChain uns eine einheitliche API um mit allen möglichen LLMs zu kommunizieren. Je nach Anwendungsfall können wir sogar die verschiedenen LLMs in einer Anwendung verwenden. So können wir mit einem Open-Source-Modell sog. Embeddings erstellen (mehr dazu in einem anderen Kapitel) und die Chat-Funktion von ChatGPT (OpenAI) nutzen. Wir können auch jederzeit das verwendete LLM austauschen, ohne den Code neu schreiben zu müssen.\n",
    "Zusätzlich dazu bietet LangChain vielfältige weitere Möglichkeiten. Mit den PromptTemplates können wir die Erstellung und Verwaltung von Prompts (Eingaben für die LLMs) bewerkstelligen, ohne irgendwelche Strings zusammen zu basteln. Wir können mit den Output Parsern die Ausgaben der LLMs maschinell auswerten und z.B. uns das Ergebnis als JSON zurück geben zu lassen, mit dem wir in unserem Programm arbeiten können. Spannend wird es spätestens dann, wenn wir die Agenten \"erwecken\". D.h. wir geben der KI Werkzeuge, mit denen die KI mehr als nur Texte erzeugen kann. Z.B. Informationen beschaffen, Berechnungen durchführen uvm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation notwendiger Bibliotheken\n",
    "\n",
    "Für die meisten Beispiele dieses Tutorial brauchen wir einige Bibliotheken. Wir brauchen natürlich die LangChain-Bibliothek. Daneben brauchen wir die OpenAI Bibliothek um mit ChatGPT zu kommunizieren (direkt aber auch unter Verwendung von LangChain). Dann benötigen wir außerdem Python Dotenv damit wir den notwendigen API-Key aus einer .env Datei laden können. Das hat den Vorteil, dass wir den geheimen API-Key nicht direkt im Code hinterlegen müssen und statt dessen in der .env Datei speichern. Diese Datei wird nicht im Git-Repository eingecheckt (in .gitignore ist diese Datei hinterlegt) und wir somit nicht aus versehen unseren API-Key veröffentlichen.\n",
    "\n",
    "Führe zunächst folgenden Befehl aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain openai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beispiel: Direkte Verwendung der OpenAI-Bibliothek\n",
    "\n",
    "Bevor wir mit LangChain starten, erstmal ein Beispiel dafür, wie man die OpenAI (ChatGPT) API direkt nutzen. Das ermöglicht uns die Verwendung zu vergleichen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"Erkläre mir bitte, wie ich gute Eingaben für ChatGPT schreibe.\"}]\n",
    "\n",
    "response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=messages)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir sehen, dass wir mit wenigen Zeilen Code eine Anfrage an ChatGPT senden können. Dabei fallen ein paar Sachen auf. Das erste ist, dass wir eine Liste von Nachrichten (_messages_) an die API übergeben. Außerdem enthält die Antwort wesentlich mehr, als nur den reinen Text der Antwort.\n",
    "\n",
    "Der Grund für die Liste der Nachrichten ist die, dass wir die Chat-Funktionalität nutzen. Der Server behält sich nicht die vorher eingegebenen Fragen und Antworten. D.h. man muss stets den gesamten Chat-Verlauf inkl. Antworten zurück übermitteln. Dadurch weiß die KI über welche Themen wir zuvor \"gesprochen\" haben. \n",
    "D.h. wenn ich jetzt eine Frage stellen möchte, die sich auf den bisherigen Chatverlauf bezieht, muss ich die Nachrichten-Liste erweitern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Die Antwort der KI muss als Nachricht in die Liste hinten angehängt werden\n",
    "messages.append(response[\"choices\"][0][\"message\"]);\n",
    "\n",
    "# Die neue Frage kommt noch dazu\n",
    "messages.append({\"role\": \"user\", \"content\": \"Danke für die Erklärung. Kannst du bitte die wesentlichen Punkte als kurze Liste zusammen fassen\"})\n",
    "\n",
    "# Erneuter Aufruf\n",
    "response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=messages)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schönere Ausgabe \n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(response[\"choices\"][0][\"message\"]['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit könnten wir schon eine Komplette Chat-Anwendung programmieren. Einfach Eingabe vom Benutzer holen, der Liste der Nachrichten hinzufügen, Aufruf ChatGPT, Antwort hinzufügen und das Ganze nochmal wiederholen.\n",
    "\n",
    "Lasst uns jetzt erste Schritte mit LangChain machen. Wir werden später sehen, wo uns LangChain hilft, diese Schritte zu automatisieren und wesentlich bequemer und übersichtlicher zu gestalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ja, Sie sprechen mit ChatGPT. Wie kann ich Ihnen helfen?\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "response = llm.predict(\"Spreche ich mit ChatGPT?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vom Code her, sind die Unterschiede zunächst nicht groß. Die Funktionen heißen etwas anders aber ansonsten wirkt das Ganze recht ähnlich. Der Unterschied ist jedoch, dass wir jederzeit das LLM austauschen können und anstelle von _ChatOpenAI_ eine andere LLM-Implementierung verwenden.\n",
    "\n",
    "In den kommenden Kapiteln sehen wir dann, wie LangChain uns die Arbeit an vielen Stellen abnimmt und uns ermöglicht das Programm sauber und übersichtlich zu halten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
