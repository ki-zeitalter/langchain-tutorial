{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory / Chat\n",
    "\n",
    "Wir haben schon das Chat-Modell von OpenAI genutzt. Allerdings haben wir das noch nicht wirklich als Chat benutzt, da die bisherigen Aufgaben stets nur ein Aufgaben + Antwortpaar produziert haben.\n",
    "Wichtig zu wissen ist, dass die ganzen Sprachmodelle, die wir für Chat-Bots nutzen, keine Daten sich behalten. D.h. der Chatverlauf existiert dort nicht. Wenn man also eine Frage stellt, die sich auf vorherige Daten aus dem Verlauf beziehen, dann weiß die KI nichts mehr darüber.\n",
    "Damit das Ganze trotzdem möglich ist, muss man stets die gesamte (oder Teile) der Konversation bei jedem neuen Prompt mitschicken. Der Chatverlauf wird dadurch zum Teil des Kontext und die KI hat ein Kurzzeitgedächtnis bekommen.\n",
    "\n",
    "Erstmal ein Beispiel, wie man das zu Fuß macht. Danach schauen wir uns an, wie man das mit LangChain automatisieren kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Du bist ein hilfreicher KI Assistent, der mir bei meinen Fragen und Aufgaben hilft\"),\n",
    "    ]\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "user_prompt = \"Erstelle bitte eine Liste der fünf größten Sprachmodelle\" # Hier könnten wir auch tatsächlich eine Eingabeaufforderung starten. Zum Zeigen gebe ich aber den Text vor\n",
    "messages.append(HumanMessage(content=user_prompt))\n",
    "\n",
    "response = llm(messages)\n",
    "messages.append(response)\n",
    "print(response)\n",
    "\n",
    "messages.append(HumanMessage(content=\"Welche stammen von OpenAI?\"))\n",
    "\n",
    "response = llm(messages)\n",
    "messages.append(response)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erstmal sehen wir, dass wir unterschiedliche Message-Typen haben: SystemMessage, Humanmessage und AIMessage. Diese widerspiegeln die verschiedenen Rollen, die von ChatGPT erwartet werden.\n",
    "Der Ablauf ist stets so, dass wir der Liste der Nachrichten die Fragen und Antworten hinzufügen. Die _messages_ Liste ist somit unser Chatverlauf und praktisch unser Speicher.\n",
    "\n",
    "Damit wir das nicht manuell machen müssen, bietet LangChain _Memory_ an. Davon gibt es unterschiedliche Implementierungen. Der einfachste Speicher merkt sich ganz einfach jede Nachricht, so wie wir es oben gemacht haben. Allerdings ist der Platz im Kontext begrenzt und ein Chat, der länger geführt wird, könnte dazu führen, dass die maximale Kontextgröße überschritten wird. Dafür gibt es Speicher, die entweder nur die letzten n Nachrichten behält oder noch ausgefeilter, ein Speicher, der die bisherige Kommunikation zusammenfasst, um Platz zu sparen.\n",
    "\n",
    "Schauen wir uns das Beispiel von oben mit LangChain Memory an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "history.add_message(SystemMessage(content=\"Du bist ein hilfreicher KI Assistent, der mir bei meinen Fragen und Aufgaben hilft\"))\n",
    "\n",
    "history.add_user_message(\"Erstelle bitte eine Liste der fünf größten Sprachmodelle\")\n",
    "\n",
    "response = llm(history.messages)\n",
    "\n",
    "history.add_ai_message(response.content)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Beispiel soll zeigen, wie die Grundstrukturen eines Speichers aussehen. Im Grunde genommen ist das mit den manuellen Schritten erstmal fast identisch und hilft uns erstmal nicht das Ganze zu automatisieren.\n",
    "In Kombination mit einer _vernünftigen_ Speicher-Implementierung und einer Chain sieht das Ganze wesentlich besser aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "response = conversation.predict(input=\"Welche Bedeutung hat die Angabe der Anzahl der Parameter bei einem Sprachmodell?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = conversation.predict(input=\"Wie viele sind es bei GPT-3?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir sehen also, dass die Chain sich automatisch darum kümmert die Nachrichten in den dazugehörigen Speicher zu laden. Durch die Verwendung von _ConversationChain_ haben wir automatisch ein Prompttemplate, das einen _input_ Parameter erwartet, den wir an die _predict_ Funktion übergeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Ausgabe der _memory_ Variable zeigt, wie die ganze Konversation mitgeschnitten wurde.\n",
    "\n",
    "Jetzt sind wir praktisch nur eine Schleife weg von einem Chatbot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "while True:\n",
    "    prompt = input(\"Prompt:\")\n",
    "    response = conversation.predict(input=prompt)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einmal hübsch machen, Weboberfläche, fertig ist der Chatbot!\n",
    "\n",
    "Jetzt kann es sein, dass wenn wir viel Eingeben (und ausgeben lassen), das irgendwann der Kontext zu voll wird. Wie oben erwähnt können wir dafür verschiedene Speicherarten nutzen.\n",
    "\n",
    "Dieser hier begrenzt den Chat auf ein bestimmtes Token-Limit (Token: Sprachmodelle arbeiten mit Tokens anstelle von Wörtern. Manche Wörter, insbesondere in Deutsch, bestehen aus mehreren Tokens.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n",
      "Der Inhalt des Speichers jetzt:\n",
      " Human: Hallo!\n",
      "AI: Hello! How can I assist you today?\n",
      "\n",
      "\n",
      "Natürlich! Hier ist eine Liste der letzten 10 Präsidenten der USA:\n",
      "\n",
      "1. Joe Biden (2021-heute)\n",
      "2. Donald Trump (2017-2021)\n",
      "3. Barack Obama (2009-2017)\n",
      "4. George W. Bush (2001-2009)\n",
      "5. Bill Clinton (1993-2001)\n",
      "6. George H.W. Bush (1989-1993)\n",
      "7. Ronald Reagan (1981-1989)\n",
      "8. Jimmy Carter (1977-1981)\n",
      "9. Gerald Ford (1974-1977)\n",
      "10. Richard Nixon (1969-1974)\n",
      "\n",
      "Das sind die letzten 10 Präsidenten der USA.\n",
      "Der Inhalt des Speichers jetzt:\n",
      " \n",
      "\n",
      "\n",
      "Der aktuelle Präsident der Vereinigten Staaten ist Joe Biden. Er wurde am 20. Januar 2021 als 46. Präsident der USA vereidigt.\n",
      "Der Inhalt des Speichers jetzt: Human: Welcher davon ist der aktuelle Präsident?\n",
      "AI: Der aktuelle Präsident der Vereinigten Staaten ist Joe Biden. Er wurde am 20. Januar 2021 als 46. Präsident der USA vereidigt.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "\n",
    "# Wir übergeben das Sprachmodell + Maximale Anzahl an Tokens. Das Sprachmodell dient dazu die Anzahl der Tokens zu ermitteln\n",
    "# Hier wir zu Demonstrationszwecken sehr kleines Limit angegeben\n",
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=100)\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "response = conversation.predict(input=\"Hallo!\")\n",
    "print(f\"Antwort: {response}\")\n",
    "print(f\"Der Inhalt des Speichers jetzt:\\n{memory.buffer_as_str}\\n\\n\")\n",
    "    \n",
    "response = conversation.predict(input=\"Erstelle mir bitte eine Liste der letzten 10 Präsidenten der USA\")\n",
    "print(f\"Antwort: {response}\")\n",
    "print(f\"Der Inhalt des Speichers jetzt:\\n{memory.buffer_as_str}\\n\\n\")\n",
    "\n",
    "response = conversation.predict(input=\"Welcher davon ist der aktuelle Präsident?\")\n",
    "print(f\"Antwort: {response}\")\n",
    "print(f\"Der Inhalt des Speichers jetzt:\\n{memory.buffer_as_str}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zu guter letzt ein schauen wir uns einen etwas ausgefeilteren Speicher an. Dieser löscht nicht einfach alte Konversationen, sondern fasst den bisherigen Verlauf zusammen.\n",
    "Zu beachten ist, dass das zusätzliche Aufrufe zu unserem Sprachmodell bedeuten und somit auch weitere Kosten verursachen kann!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hallo!\n",
      "AI:\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human greets the AI and the AI asks how it can assist the human.\n",
      "Human: Erstelle mir bitte eine Liste der letzten 10 Präsidenten der USA\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The AI asks how it can assist the human. The human requests a list of the last 10 presidents of the USA. The AI provides the requested list and asks if there is anything else it can help with.\n",
      "Human: Welcher davon ist der aktuelle Präsident?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "\n",
    "# Wir übergeben das Sprachmodell. Das Sprachmodell dient dazu die Zusammenfassung zu erstellen.\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "response = conversation.predict(input=\"Hallo!\")\n",
    "print(f\"Antwort: {response}\")\n",
    "    \n",
    "response = conversation.predict(input=\"Erstelle mir bitte eine Liste der letzten 10 Präsidenten der USA\")\n",
    "print(f\"Antwort: {response}\")\n",
    "\n",
    "response = conversation.predict(input=\"Welcher davon ist der aktuelle Präsident?\")\n",
    "print(f\"Antwort: {response}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
