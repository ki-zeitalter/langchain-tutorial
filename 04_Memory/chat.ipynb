{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory / Chat\n",
    "\n",
    "Wir haben schon das Chat-Modell von OpenAI genutzt. Allerdings haben wir das noch nicht wirklich als Chat benutzt, da die bisherigen Aufgaben stets nur ein Aufgaben + Antwortpaar produziert haben.\n",
    "Wichtig zu wissen ist, dass die ganzen Sprachmodelle, die wir für Chat-Bots nutzen, keine Daten sich behalten. D.h. der Chatverlauf existiert dort nicht. Wenn man also eine Frage stellt, die sich auf vorherige Daten aus dem Verlauf beziehen, dann weiß die KI nichts mehr darüber.\n",
    "Damit das Ganze trotzdem möglich ist, muss man stets die gesamte (oder Teile) der Konversation bei jedem neuen Prompt mitschicken. Der Chatverlauf wird dadurch zum Teil des Kontext und die KI hat ein Kurzzeitgedächtnis bekommen.\n",
    "\n",
    "Erstmal ein Beispiel, wie man das zu Fuß macht. Danach schauen wir uns an, wie man das mit LangChain automatisieren kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='1. GPT-3 (Generative Pre-trained Transformer 3) - Entwickelt von OpenAI, hat 175 Milliarden Parameter und gilt momentan als das größte Sprachmodell.\\n\\n2. GPT-2 (Generative Pre-trained Transformer 2) - Auch von OpenAI entwickelt, hat 1,5 Milliarden Parameter und war vor GPT-3 das größte verfügbare Sprachmodell.\\n\\n3. T5 (Text-to-Text Transfer Transformer) - Entwickelt von Google Research, hat 11 Milliarden Parameter und wurde für eine Vielzahl von Aufgaben wie Übersetzung, Textgenerierung und Frage-Antwort verwendet.\\n\\n4. Turing NLG - Entwickelt von Microsoft Research, hat 17 Milliarden Parameter und wurde speziell für die natürliche Sprachgenerierung entwickelt.\\n\\n5. Megatron-LM - Entwickelt von NVIDIA, hat 8,3 Milliarden Parameter und wurde für verschiedene Aufgaben im Bereich des maschinellen Lernens eingesetzt, darunter auch die Sprachverarbeitung.'\n",
      "content='Von den genannten Sprachmodellen stammen GPT-3 und GPT-2 von OpenAI. OpenAI ist ein Unternehmen für künstliche Intelligenz und hat sich auf die Entwicklung von fortschrittlichen Sprachmodellen spezialisiert. GPT-3 und GPT-2 sind zwei ihrer bekanntesten und größten Sprachmodelle.'\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Du bist ein hilfreicher KI Assistent, der mir bei meinen Fragen und Aufgaben hilft\"),\n",
    "    ]\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "user_prompt = \"Erstelle bitte eine Liste der fünf größten Sprachmodelle\" # Hier könnten wir auch tatsächlich eine Eingabeaufforderung starten. Zum Zeigen gebe ich aber den Text vor\n",
    "messages.append(HumanMessage(content=user_prompt))\n",
    "\n",
    "response = llm(messages)\n",
    "messages.append(response)\n",
    "print(response)\n",
    "\n",
    "messages.append(HumanMessage(content=\"Welche stammen von OpenAI?\"))\n",
    "\n",
    "response = llm(messages)\n",
    "messages.append(response)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erstmal sehen wir, dass wir unterschiedliche Message-Typen haben: SystemMessage, Humanmessage und AIMessage. Diese widerspiegeln die verschiedenen Rollen, die von ChatGPT erwartet werden.\n",
    "Der Ablauf ist stets so, dass wir der Liste der Nachrichten die Fragen und Antworten hinzufügen. Die _messages_ Liste ist somit unser Chatverlauf und praktisch unser Speicher.\n",
    "\n",
    "Damit wir das nicht manuell machen müssen, bietet LangChain _Memory_ an. Davon gibt es unterschiedliche Implementierungen. Der einfachste Speicher merkt sich ganz einfach jede Nachricht, so wie wir es oben gemacht haben. Allerdings ist der Platz im Kontext begrenzt und ein Chat, der länger geführt wird, könnte dazu führen, dass die maximale Kontextgröße überschritten wird. Dafür gibt es Speicher, die entweder nur die letzten n Nachrichten behält oder noch ausgefeilter, ein Speicher, der die bisherige Kommunikation zusammenfasst, um Platz zu sparen.\n",
    "\n",
    "Schauen wir uns das Beispiel von oben mit LangChain Memory an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hier ist eine Liste der fünf größten Sprachmodelle:\\n\\n1. GPT-3 (Generative Pretrained Transformer 3) - Entwickelt von OpenAI, besteht GPT-3 aus 175 Milliarden Parametern und gilt derzeit als eines der größten Sprachmodelle der Welt.\\n\\n2. Turing NLG (Natural Language Generation) - Entwickelt von Microsoft, basiert auf GPT-3 und hat ebenfalls eine beeindruckende Größe von 17 Milliarden Parametern.\\n\\n3. T5 (Text-to-Text Transfer Transformer) - Entwickelt von Google, besteht T5 aus 11 Milliarden Parametern und zeichnet sich durch seine Fähigkeit aus, Texte in verschiedene Sprachen zu übersetzen.\\n\\n4. Megatron-LM - Entwickelt von NVIDIA, ist Megatron-LM ein Sprachmodell mit 8,3 Milliarden Parametern und wurde speziell für die Verarbeitung von natürlicher Sprache entwickelt.\\n\\n5. ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately) - Entwickelt von Google, hat ELECTRA rund 134 Millionen Parameter und zeichnet sich durch seine effiziente Trainingsmethode aus.\\n\\nBitte beachte, dass sich die Größe und die Rangfolge dieser Modelle mit der Zeit ändern können, da ständig neue Modelle entwickelt und veröffentlicht werden.'\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "history.add_message(SystemMessage(content=\"Du bist ein hilfreicher KI Assistent, der mir bei meinen Fragen und Aufgaben hilft\"))\n",
    "\n",
    "history.add_user_message(\"Erstelle bitte eine Liste der fünf größten Sprachmodelle\")\n",
    "\n",
    "response = llm(history.messages)\n",
    "\n",
    "history.add_ai_message(response.content)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Beispiel soll zeigen, wie die Grundstrukturen eines Speichers aussehen. Im Grunde genommen ist das mit den manuellen Schritten erstmal fast identisch und hilft uns erstmal nicht das Ganze zu automatisieren.\n",
    "In Kombination mit einer _vernünftigen_ Speicher-Implementierung und einer Chain sieht das Ganze wesentlich besser aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Angabe der Anzahl der Parameter bei einem Sprachmodell gibt Auskunft über die Komplexität und Größe des Modells. Ein Parameter ist eine Variable, die das Modell verwendet, um Informationen zu speichern oder zu verarbeiten. Je mehr Parameter ein Modell hat, desto mehr Informationen kann es verarbeiten und desto komplexere Zusammenhänge kann es erfassen. Die Anzahl der Parameter kann auch einen Einfluss auf die Leistung des Modells haben. Ein Modell mit mehr Parametern kann in der Regel mehr Daten und Informationen berücksichtigen, was zu einer höheren Genauigkeit führen kann. Allerdings kann ein Modell mit zu vielen Parametern auch zu Overfitting führen, bei dem das Modell die Trainingsdaten zu stark \"auswendig lernt\" und bei neuen Daten schlechter abschneidet. Es ist also wichtig, ein ausgewogenes Verhältnis zwischen der Anzahl der Parameter und der Leistung des Modells zu finden.\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "response = conversation.predict(input=\"Welche Bedeutung hat die Angabe der Anzahl der Parameter bei einem Sprachmodell?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-3 hat insgesamt 175 Milliarden Parameter. Das ist eine enorme Anzahl und macht GPT-3 zu einem der größten und komplexesten Sprachmodelle, die bisher entwickelt wurden. Mit dieser großen Anzahl an Parametern kann GPT-3 eine breite Palette an Informationen und Zusammenhängen erfassen und komplexe Aufgaben im Bereich der natürlichen Sprachverarbeitung bewältigen.\n"
     ]
    }
   ],
   "source": [
    "response = conversation.predict(input=\"Wie viele sind es bei GPT-3?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir sehen also, dass die Chain sich automatisch darum kümmert die Nachrichten in den dazugehörigen Speicher zu laden. Durch die Verwendung von _ConversationChain_ haben wir automatisch ein Prompttemplate, das einen _input_ Parameter erwartet, den wir an die _predict_ Funktion übergeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='Welche Bedeutung hat die Angabe der Anzahl der Parameter bei einem Sprachmodell?'), AIMessage(content='Die Angabe der Anzahl der Parameter bei einem Sprachmodell gibt Auskunft über die Komplexität und Größe des Modells. Ein Parameter ist eine Variable, die das Modell verwendet, um Informationen zu speichern oder zu verarbeiten. Je mehr Parameter ein Modell hat, desto mehr Informationen kann es verarbeiten und desto komplexere Zusammenhänge kann es erfassen. Die Anzahl der Parameter kann auch einen Einfluss auf die Leistung des Modells haben. Ein Modell mit mehr Parametern kann in der Regel mehr Daten und Informationen berücksichtigen, was zu einer höheren Genauigkeit führen kann. Allerdings kann ein Modell mit zu vielen Parametern auch zu Overfitting führen, bei dem das Modell die Trainingsdaten zu stark \"auswendig lernt\" und bei neuen Daten schlechter abschneidet. Es ist also wichtig, ein ausgewogenes Verhältnis zwischen der Anzahl der Parameter und der Leistung des Modells zu finden.'), HumanMessage(content='Wie viele sind es bei GPT-3?'), AIMessage(content='GPT-3 hat insgesamt 175 Milliarden Parameter. Das ist eine enorme Anzahl und macht GPT-3 zu einem der größten und komplexesten Sprachmodelle, die bisher entwickelt wurden. Mit dieser großen Anzahl an Parametern kann GPT-3 eine breite Palette an Informationen und Zusammenhängen erfassen und komplexe Aufgaben im Bereich der natürlichen Sprachverarbeitung bewältigen.')]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Ausgabe der _memory_ Variable zeigt, wie die ganze Konversation mitgeschnitten wurde.\n",
    "\n",
    "Jetzt sind wir praktisch nur eine Schleife weg von einem Chatbot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mir geht es gut, danke! Als KI habe ich keine Gefühle im eigentlichen Sinne, aber ich bin bereit, mit dir zu chatten und zu helfen. Wie kann ich dir behilflich sein?\n",
      "Natürlich! Ich kann dir gerne alles über KIs erklären. Künstliche Intelligenz ist ein Bereich der Informatik, der sich mit der Entwicklung von Systemen befasst, die in der Lage sind, Aufgaben auszuführen, die normalerweise menschliche Intelligenz erfordern würden. KIs verwenden Algorithmen und Daten, um Muster zu erkennen, Entscheidungen zu treffen und Probleme zu lösen.\n",
      "\n",
      "Es gibt verschiedene Arten von KIs, wie zum Beispiel schwache KIs und starke KIs. Schwache KIs sind auf spezifische Aufgaben oder Probleme spezialisiert, wie zum Beispiel Gesichtserkennung oder Sprachverarbeitung. Starke KIs hingegen sind in der Lage, eine Vielzahl von Aufgaben zu erledigen und sogar menschenähnliche Intelligenz zu zeigen.\n",
      "\n",
      "Künstliche Intelligenz hat viele Anwendungen in verschiedenen Bereichen wie Medizin, Finanzen, Verkehr, Robotik und vielem mehr. Zum Beispiel werden KIs in der Medizin eingesetzt, um Krankheiten zu diagnostizieren oder medizinische Daten zu analysieren.\n",
      "\n",
      "Es gibt auch ethische und soziale Fragen im Zusammenhang mit KIs. Einige Menschen befürchten, dass KIs Arbeitsplätze übernehmen könnten oder dass sie möglicherweise außer Kontrolle geraten könnten. Es ist wichtig, diese Fragen zu berücksichtigen und sicherzustellen, dass KIs verantwortungsvoll eingesetzt werden.\n",
      "\n",
      "Ich hoffe, das gibt dir einen guten Überblick über KIs! Wenn du noch weitere Fragen hast, stehe ich gerne zur Verfügung.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/langchain-tutorial/04_Memory/chat.ipynb Zelle 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bminiature-zebra-ww6g7jp967gf9656/workspaces/langchain-tutorial/04_Memory/chat.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m conversation \u001b[39m=\u001b[39m ConversationChain(\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bminiature-zebra-ww6g7jp967gf9656/workspaces/langchain-tutorial/04_Memory/chat.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     llm\u001b[39m=\u001b[39mllm, \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bminiature-zebra-ww6g7jp967gf9656/workspaces/langchain-tutorial/04_Memory/chat.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     memory\u001b[39m=\u001b[39mmemory\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bminiature-zebra-ww6g7jp967gf9656/workspaces/langchain-tutorial/04_Memory/chat.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bminiature-zebra-ww6g7jp967gf9656/workspaces/langchain-tutorial/04_Memory/chat.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://codespaces%2Bminiature-zebra-ww6g7jp967gf9656/workspaces/langchain-tutorial/04_Memory/chat.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     prompt \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mPrompt:\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bminiature-zebra-ww6g7jp967gf9656/workspaces/langchain-tutorial/04_Memory/chat.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     response \u001b[39m=\u001b[39m conversation\u001b[39m.\u001b[39mpredict(\u001b[39minput\u001b[39m\u001b[39m=\u001b[39mprompt)\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bminiature-zebra-ww6g7jp967gf9656/workspaces/langchain-tutorial/04_Memory/chat.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mprint\u001b[39m(response)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py:1202\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1200\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1201\u001b[0m     \u001b[39mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1202\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_input_request(\n\u001b[1;32m   1203\u001b[0m     \u001b[39mstr\u001b[39;49m(prompt),\n\u001b[1;32m   1204\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parent_ident[\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m   1205\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_parent(\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1206\u001b[0m     password\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1207\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py:1245\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[39m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mInterrupted by user\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1245\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1246\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1247\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39mInvalid Message:\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "while True:\n",
    "    prompt = input(\"Prompt:\")\n",
    "    response = conversation.predict(input=prompt)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einmal hübsch machen, Weboberfläche, fertig ist der Chatbot!\n",
    "\n",
    "Jetzt kann es sein, dass wenn wir viel Eingeben (und ausgeben lassen), das irgendwann der Kontext zu voll wird. Wie oben erwähnt können wir dafür verschiedene Speicherarten nutzen.\n",
    "\n",
    "Dieser hier begrenzt den Chat auf ein bestimmtes Token-Limit (Token: Sprachmodelle arbeiten mit Tokens anstelle von Wörtern. Manche Wörter, insbesondere in Deutsch, bestehen aus mehreren Tokens.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antwort: Hello! How can I assist you today?\n",
      "Der Inhalt des Speichers jetzt:\n",
      "Human: Hallo!\n",
      "AI: Hello! How can I assist you today?\n",
      "\n",
      "\n",
      "Antwort: Natürlich! Hier sind die letzten 10 Präsidenten der USA:\n",
      "\n",
      "1. Joe Biden (2021-heute)\n",
      "2. Donald Trump (2017-2021)\n",
      "3. Barack Obama (2009-2017)\n",
      "4. George W. Bush (2001-2009)\n",
      "5. Bill Clinton (1993-2001)\n",
      "6. George H. W. Bush (1989-1993)\n",
      "7. Ronald Reagan (1981-1989)\n",
      "8. Jimmy Carter (1977-1981)\n",
      "9. Gerald Ford (1974-1977)\n",
      "10. Richard Nixon (1969-1974)\n",
      "\n",
      "Gibt es noch etwas, womit ich Ihnen helfen kann?\n",
      "Der Inhalt des Speichers jetzt:\n",
      "\n",
      "\n",
      "\n",
      "Antwort: Der aktuelle Präsident ist Joe Biden. Er wurde am 20. Januar 2021 als 46. Präsident der Vereinigten Staaten vereidigt. Er ist Mitglied der Demokratischen Partei und diente zuvor als Vizepräsident unter Barack Obama. Biden hat eine lange politische Karriere hinter sich und war auch acht Jahre lang Senator für Delaware.\n",
      "Der Inhalt des Speichers jetzt:\n",
      "AI: Der aktuelle Präsident ist Joe Biden. Er wurde am 20. Januar 2021 als 46. Präsident der Vereinigten Staaten vereidigt. Er ist Mitglied der Demokratischen Partei und diente zuvor als Vizepräsident unter Barack Obama. Biden hat eine lange politische Karriere hinter sich und war auch acht Jahre lang Senator für Delaware.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "\n",
    "# Wir übergeben das Sprachmodell + Maximale Anzahl an Tokens. Das Sprachmodell dient dazu die Anzahl der Tokens zu ermitteln\n",
    "# Hier wir zu Demonstrationszwecken sehr kleines Limit angegeben\n",
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=100)\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "response = conversation.predict(input=\"Hallo!\")\n",
    "print(f\"Antwort: {response}\")\n",
    "print(f\"Der Inhalt des Speichers jetzt:\\n{memory.buffer_as_str}\\n\\n\")\n",
    "    \n",
    "response = conversation.predict(input=\"Erstelle mir bitte eine Liste der letzten 10 Präsidenten der USA\")\n",
    "print(f\"Antwort: {response}\")\n",
    "print(f\"Der Inhalt des Speichers jetzt:\\n{memory.buffer_as_str}\\n\\n\")\n",
    "\n",
    "response = conversation.predict(input=\"Welcher davon ist der aktuelle Präsident?\")\n",
    "print(f\"Antwort: {response}\")\n",
    "print(f\"Der Inhalt des Speichers jetzt:\\n{memory.buffer_as_str}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zu guter letzt ein schauen wir uns einen etwas ausgefeilteren Speicher an. Dieser löscht nicht einfach alte Konversationen, sondern fasst den bisherigen Verlauf zusammen.\n",
    "Zu beachten ist, dass das zusätzliche Aufrufe zu unserem Sprachmodell bedeuten und somit auch weitere Kosten verursachen kann!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hallo!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Antwort: Hallo! Wie kann ich Ihnen helfen?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human greets the AI in German. The AI responds in German and asks how it can help.\n",
      "Human: Erstelle mir bitte eine Liste der letzten 10 Präsidenten der USA\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Antwort: Hallo! Natürlich kann ich das für dich tun. Hier ist eine Liste der letzten 10 Präsidenten der USA:\n",
      "\n",
      "1. Joe Biden (2021 - heute)\n",
      "2. Donald Trump (2017 - 2021)\n",
      "3. Barack Obama (2009 - 2017)\n",
      "4. George W. Bush (2001 - 2009)\n",
      "5. Bill Clinton (1993 - 2001)\n",
      "6. George H. W. Bush (1989 - 1993)\n",
      "7. Ronald Reagan (1981 - 1989)\n",
      "8. Jimmy Carter (1977 - 1981)\n",
      "9. Gerald Ford (1974 - 1977)\n",
      "10. Richard Nixon (1969 - 1974)\n",
      "\n",
      "Gibt es noch etwas, womit ich dir helfen kann?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human greets the AI in German and asks it to create a list of the last 10 presidents of the USA. The AI responds in German and provides the requested list. It then asks if there is anything else it can help with.\n",
      "Human: Welcher davon ist der aktuelle Präsident?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Antwort: Der aktuelle Präsident der USA ist Joe Biden. Kann ich Ihnen sonst noch weiterhelfen?\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "\n",
    "# Wir übergeben das Sprachmodell. Das Sprachmodell dient dazu die Zusammenfassung zu erstellen.\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "response = conversation.predict(input=\"Hallo!\")\n",
    "print(f\"Antwort: {response}\")\n",
    "    \n",
    "response = conversation.predict(input=\"Erstelle mir bitte eine Liste der letzten 10 Präsidenten der USA\")\n",
    "print(f\"Antwort: {response}\")\n",
    "\n",
    "response = conversation.predict(input=\"Welcher davon ist der aktuelle Präsident?\")\n",
    "print(f\"Antwort: {response}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
